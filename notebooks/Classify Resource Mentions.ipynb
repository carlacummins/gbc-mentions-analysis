{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVW8w8mmKnVy"
   },
   "source": [
    "# üß† GBC SciBERT Resource Mention Classifier Testing\n",
    "This notebook uses GBC's fine-tuned version of the SciBERT model to identify mentions of biodata resources and classify them as true/false mentions. Input a pubmed ID or a PMC ID to extract potential biodata resource mentions and classify them.\n",
    "\n",
    "### Usage instructions\n",
    "- On fresh instance, press the 'Run all' button in the menu. This will perform all setup steps and then run classification.\n",
    "- On an already-running instance, update your publication id below and run the cell. Then, from the 'Table of contents' menu, click the 3 dots next to 'üß† Run Predictions' section and select `Run cells in section`. This avoids rerunning setup each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1751532081769,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "tkKZM_sqNig4"
   },
   "outputs": [],
   "source": [
    "# @title üìÑ Set publication ID\n",
    "pmid = \"\" # @param {\"type\":\"string\",\"placeholder\":\"None\"}\n",
    "pmcid = \"\" # @param {\"type\":\"string\", \"placeholder\":\"None\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJYAfi05OGtu"
   },
   "source": [
    "# ‚öôÔ∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 101705,
     "status": "ok",
     "timestamp": 1751532188739,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "HAGbzln-OFYX",
    "outputId": "ed839c8f-36c4-4d16-ee46-3c278969b361"
   },
   "outputs": [],
   "source": [
    "# @title üì¶ Install dependencies\n",
    "%pip install pandas requests beautifulsoup4\n",
    "%pip install nltk transformers torch\n",
    "%pip install sqlalchemy pymysql tqdm\n",
    "%pip install itables ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4823,
     "status": "ok",
     "timestamp": 1751532193559,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "PI7UG2TFe32G",
    "outputId": "141a5383-318c-4fdf-a9d8-df6db990a975"
   },
   "outputs": [],
   "source": [
    "# @title ‚¨áÔ∏è Download tokenizers\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23916,
     "status": "ok",
     "timestamp": 1751532217478,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "NZSUu2hJOyMg"
   },
   "outputs": [],
   "source": [
    "# @title üß© Import modules\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sqlalchemy as db\n",
    "# import pymysql\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import Markdown, HTML\n",
    "from itables import show\n",
    "import itables.options as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "opt.classes = \"display nowrap\"\n",
    "opt.lengthMenu = [25, 50, 100]\n",
    "opt.column_filters = \"header\"\n",
    "opt.maxBytes = 0  # Show full strings\n",
    "opt.scrollX = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-hPBFHRWnEw"
   },
   "source": [
    "# üì• Pull resource list from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 140,
     "status": "aborted",
     "timestamp": 1751532339080,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "t2gk2WxaPg9B"
   },
   "outputs": [],
   "source": [
    "# @title üîé DB connection setup (via public IP)\n",
    "db_engine = db.create_engine('mysql+pymysql://gbcreader@34.89.127.34/gbc-publication-analysis', pool_recycle=3600, pool_size=50, max_overflow=50)\n",
    "db_conn = db_engine.connect()\n",
    "\n",
    "print(\"Successfully connected to GBC MySQL instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 225,
     "status": "aborted",
     "timestamp": 1751532339169,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "ooMyb-uPT0Yo"
   },
   "outputs": [],
   "source": [
    "# @title üìã Load resource list\n",
    "\n",
    "def display_dataframe(df, title=None):\n",
    "    if title:\n",
    "        print(f\"### {title} ###\")\n",
    "\n",
    "    display(HTML(\"<div style='max-width: 800px; overflow-x: auto;'>\"))\n",
    "    show(df, include_index=False, classes=\"display compact\", style=\"width: 800px;\")\n",
    "    display(HTML(\"</div>\"))\n",
    "\n",
    "additional_aliases = json.load(open(\"resource_names.additional_aliases.json\"))\n",
    "\n",
    "sql = \"SELECT short_name, common_name, full_name FROM resource WHERE is_latest=1\"\n",
    "result = db_conn.execute(db.text(sql)).fetchall()\n",
    "resource_names = []\n",
    "for r in result:\n",
    "    short_name = r[0].strip()\n",
    "    common_name = r[1].strip() if r[1] else None\n",
    "    full_name = r[2].strip() if r[2] else None\n",
    "    if short_name:\n",
    "        resource_names.append([short_name])\n",
    "    if common_name and common_name != short_name:\n",
    "        resource_names[-1].append(common_name)\n",
    "    if full_name and full_name != short_name and full_name != common_name:\n",
    "        resource_names[-1].append(full_name)\n",
    "\n",
    "    if short_name in additional_aliases:\n",
    "        resource_names[-1].extend(additional_aliases[short_name])\n",
    "\n",
    "resource_display_df = pd.DataFrame([\n",
    "    {\n",
    "        'resource_name': entry[0],\n",
    "        'aliases': str(entry)\n",
    "    }\n",
    "    for entry in resource_names\n",
    "])\n",
    "display_dataframe(resource_display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmq65xvwXYxT"
   },
   "source": [
    "# üì¶ Locate and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 169,
     "status": "aborted",
     "timestamp": 1751462673129,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "xDrrxEuzTiSv"
   },
   "outputs": [],
   "source": [
    "# grab model files from GDrive\n",
    "!cp -r '/content/drive/MyDrive/Colab Notebooks/SciBERT Classifier/scibert_resource_classifier' .\n",
    "!cp -r '/content/drive/MyDrive/Colab Notebooks/SciBERT Classifier/scibert_resource_classifier.v2' .\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.set_num_threads(2)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"scibert_resource_classifier.v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"scibert_resource_classifier.v2\").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDevosYzXrir"
   },
   "source": [
    "# üß∞ Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 166,
     "status": "aborted",
     "timestamp": 1751462673130,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "BAscsVbyODO-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# setup retry strategy and HTTP adapter to handle rate limiting\n",
    "# and transient errors\n",
    "retry_strategy = Retry(\n",
    "    total=5,                      # Try up to 5 times\n",
    "    backoff_factor=1.5,           # Starts with 1.5s ‚Üí 3s ‚Üí 6s ‚Üí 12s ‚Üí 24s\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "    raise_on_status=False\n",
    ")\n",
    "\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", adapter)\n",
    "session.mount(\"http://\", adapter)\n",
    "\n",
    "# query EuropePMC for publication metadata\n",
    "max_retries = 5\n",
    "epmc_base_url = \"https://www.ebi.ac.uk/europepmc/webservices/rest\"\n",
    "\n",
    "def query_europepmc(endpoint, request_params=None, no_exit=False):\n",
    "    \"\"\"\n",
    "    Query Europe PMC REST API endpoint with retries.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = session.get(endpoint, params=request_params, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                return response.json() if 'json' in response.headers.get('Content-Type', '') else response.text\n",
    "            else:\n",
    "                if no_exit:\n",
    "                    return None\n",
    "                else:\n",
    "                    sys.exit(f\"Error: {response.status_code} for {endpoint}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"‚ö†Ô∏è Request failed: {e}. Retrying ({attempt + 1}/{max_retries})...\")\n",
    "    sys.exit(\"Max retries exceeded.\")\n",
    "\n",
    "def preprocess_xml_table(table_wrap_tag):\n",
    "    \"\"\"Extracts and flattens a single <table-wrap> tag into a list of text lines suitable for NER.\"\"\"\n",
    "    lines = []\n",
    "\n",
    "    # Caption\n",
    "    caption = table_wrap_tag.find(\"caption\")\n",
    "    if caption:\n",
    "        cap_text = caption.get_text(strip=True)\n",
    "        if cap_text:\n",
    "            lines.append(f\"[TABLE-CAPTION] {cap_text}\")\n",
    "\n",
    "    # Table body\n",
    "    table = table_wrap_tag.find(\"table\")\n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for i, row in enumerate(rows):\n",
    "            cells = row.find_all([\"td\", \"th\"])\n",
    "            if cells:\n",
    "                row_text = []\n",
    "                for cell in cells:\n",
    "                    text = cell.get_text(strip=True)\n",
    "                    if text:\n",
    "                        is_header = cell.name == \"th\" or i == 0\n",
    "                        prefix = \"[COLUMN-HEADER] \" if is_header else \"\"\n",
    "                        row_text.append(f\"{prefix}{text}\")\n",
    "                if row_text:\n",
    "                    lines.append(\" \".join(row_text))\n",
    "\n",
    "    return \"\\n\".join(lines) if lines else None\n",
    "\n",
    "def section_to_text(section, depth=1):\n",
    "    \"\"\"Converts a BeautifulSoup section to a string.\"\"\"\n",
    "    text = []\n",
    "    title = section.find(\"title\", recursive=False)\n",
    "    if title:\n",
    "        text.append(f\"{'#'*depth} {title.get_text(strip=True).upper()}\")\n",
    "\n",
    "    elems = section.find_all([\"sec\", \"p\"], recursive=False) # only direct children\n",
    "    for elem in elems:\n",
    "        if elem.name == \"sec\":\n",
    "            text.append(section_to_text(elem, depth=(depth+1)))\n",
    "        elif elem.name == \"p\":\n",
    "            # check for embedded lists\n",
    "            plists = elem.find_all(\"list\", recursive=False)\n",
    "            for plist in plists:\n",
    "                for li in elem.find_all(\"list-item\", recursive=True):\n",
    "                    li_text = li.get_text(strip=True)\n",
    "                    if li_text:\n",
    "                        text.append(f\"- {li_text}.\")\n",
    "\n",
    "                plist.extract() # remove the lists from the main paragraph\n",
    "\n",
    "            p_text = elem.get_text(strip=True)\n",
    "            if p_text:\n",
    "                text.append(p_text)\n",
    "\n",
    "    return \"\\n\".join(text) if text else ''\n",
    "\n",
    "def get_fulltext_body(pmcid):\n",
    "    # 1. Download the XML\n",
    "    url = f\"{epmc_base_url}/{pmcid}/fullTextXML\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    xml = response.text\n",
    "\n",
    "    # 2. Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(xml, \"lxml-xml\")\n",
    "\n",
    "    # 3. Extract body text with headers\n",
    "    text_blocks = []\n",
    "\n",
    "    # 1. Title\n",
    "    title = soup.find(\"article-title\")\n",
    "    if title:\n",
    "        title_text = title.get_text(strip=True)\n",
    "        if title_text:\n",
    "            text_blocks.append(f\"# TITLE\\n{title_text}\")\n",
    "    text_blocks.append(\"\\n\")\n",
    "\n",
    "    # 2. Abstract\n",
    "    abstract = soup.find(\"abstract\")\n",
    "    if abstract:\n",
    "        abstract_title = abstract.find(\"title\")\n",
    "        if abstract_title and abstract_title.get_text(strip=True).upper() == 'ABSTRACT':\n",
    "            abstract_title.extract()  # remove the title\n",
    "\n",
    "        text_blocks.append(f\"# ABSTRACT\\n{section_to_text(abstract)}\")\n",
    "\n",
    "    # 2.1. Other metadata sections\n",
    "    funding_statement = soup.find(\"funding-statement\")\n",
    "    if funding_statement:\n",
    "        funding_text = funding_statement.get_text(strip=True)\n",
    "        if funding_text:\n",
    "            text_blocks.append(f\"### FUNDING\\n{funding_text}\")\n",
    "\n",
    "    all_custom_metas = soup.find_all(\"custom-meta\")\n",
    "    for custom_meta in all_custom_metas:\n",
    "        meta_name = custom_meta.find(\"meta-name\").get_text(strip=True)\n",
    "        meta_value = custom_meta.find(\"meta-value\").get_text(strip=True)\n",
    "        if meta_name and meta_value:\n",
    "            text_blocks.append(f\"### {meta_name.upper()}\\n{meta_value}\")\n",
    "\n",
    "    text_blocks.append(\"\\n\")\n",
    "\n",
    "    # 3. Tables (captions + content)\n",
    "    table_blocks = []\n",
    "    for tbl in soup.find_all(\"table-wrap\"):\n",
    "        tbl.extract()\n",
    "        processed_table = preprocess_xml_table(tbl)\n",
    "        if processed_table:\n",
    "            table_blocks.append(processed_table)\n",
    "\n",
    "    # 4. Main body (sections + paragraphs)\n",
    "    # excluded_section_types = [\"supplementary-material\", \"orcid\"]\n",
    "    excluded_section_types = [\"orcid\"]\n",
    "    body = soup.find(\"body\")\n",
    "    if body:\n",
    "        all_sections = body.find_all(\"sec\", recursive=False)\n",
    "        for elem in all_sections:\n",
    "            if elem.get(\"sec-type\") in excluded_section_types:\n",
    "                continue\n",
    "\n",
    "            text_blocks.append(section_to_text(elem))\n",
    "            text_blocks.append(\"\\n\")\n",
    "\n",
    "    return text_blocks, table_blocks\n",
    "\n",
    "def remove_substring_matches(mentions):\n",
    "    aliases = [m[1].lower() for m in mentions]\n",
    "    unique_aliases = list(set(aliases))\n",
    "\n",
    "    substr_aliases = []\n",
    "    for alias1 in unique_aliases:\n",
    "        for alias2 in unique_aliases:\n",
    "            if alias1 in alias2 and alias1 != alias2:\n",
    "                substr_aliases.append(alias1)\n",
    "\n",
    "    for alias in substr_aliases:\n",
    "        mentions = [m for m in mentions if m[1].lower() != alias]\n",
    "\n",
    "    return mentions\n",
    "\n",
    "case_sensitive_threshold = 30 # switch to case sensitive search after this number of matches for a resource\n",
    "def get_resource_mentions(textblocks, tableblocks, resource_names):\n",
    "    mentions = []\n",
    "\n",
    "    # precompile regex patterns for each resource alias\n",
    "    # This is more efficient than compiling them on-the-fly in the loop\n",
    "    compiled_patterns = []\n",
    "    for resource in resource_names:\n",
    "        resource_name = resource[0]\n",
    "        for alias in resource:\n",
    "            pattern_case_insensitive = re.compile(rf\"[^A-Za-z]{re.escape(alias.lower())}[^A-Za-z]\")\n",
    "            compiled_patterns.append((resource_name, alias, pattern_case_insensitive))\n",
    "\n",
    "    # Split the fulltext into sentences and table rows\n",
    "    for block in textblocks:\n",
    "        # sentences = block.split('. ')\n",
    "        sentences = sent_tokenize(block)  # Use NLTK to split into sentences\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.replace(\"\\n\", \" \")\n",
    "            s_lowered = sentence.lower()\n",
    "            this_sentence_mentions = []\n",
    "            for resource_name, alias, pattern_ci in compiled_patterns:\n",
    "                if pattern_ci.search(s_lowered):\n",
    "                    this_sentence_mentions.append((sentence.strip(), alias, resource_name))\n",
    "\n",
    "            if len(this_sentence_mentions) > 1:\n",
    "                this_sentence_mentions = remove_substring_matches(this_sentence_mentions)\n",
    "            mentions.extend(this_sentence_mentions)\n",
    "\n",
    "    for table in tableblocks:\n",
    "        rows = table.split('\\n')\n",
    "\n",
    "        for row in rows:\n",
    "            r_lowered = row.lower()\n",
    "            this_row_mentions = []\n",
    "            for resource_name, alias, pattern_ci in compiled_patterns:\n",
    "                if pattern_ci.search(r_lowered):\n",
    "                    this_row_mentions.append((row.strip(), alias, resource_name))\n",
    "\n",
    "            if len(this_row_mentions) > 1:\n",
    "                this_row_mentions = remove_substring_matches(this_row_mentions)\n",
    "            mentions.extend(this_row_mentions)\n",
    "\n",
    "    # if a large number of matches are found for one resource, switch to case sensitive mode\n",
    "    filtered_mentions = []\n",
    "    alias_counts = Counter([m[1] for m in mentions])\n",
    "    for alias, count in alias_counts.items():\n",
    "        if count > case_sensitive_threshold:\n",
    "            print(f\"‚ö†Ô∏è {count} matches found for {alias} - switching to case sensitive mode\")\n",
    "            pattern_case_sensitive = re.compile(rf\"[^A-Za-z]{re.escape(alias)}[^A-Za-z]\")\n",
    "            for m in mentions:\n",
    "                if m[1] == alias and pattern_case_sensitive.search(m[0]):\n",
    "                    filtered_mentions.append(m)\n",
    "        else:\n",
    "            this_alias_mentions = [m for m in mentions if m[1] == alias]\n",
    "            filtered_mentions.extend(this_alias_mentions)\n",
    "\n",
    "    # Remove duplicates\n",
    "    mentions = list(set(filtered_mentions))\n",
    "    # Remove empty mentions\n",
    "    mentions = [m for m in mentions if m[0]]\n",
    "\n",
    "    return mentions\n",
    "\n",
    "def classify_mentions(pmcid, pmid, candidate_pairs):\n",
    "    predictions = []\n",
    "\n",
    "    for sentence, alias, resource in tqdm(candidate_pairs, desc=\"üîç Classifying\"):\n",
    "        inputs = tokenizer(alias, sentence, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            pred = torch.argmax(probs, dim=1).item()\n",
    "            if pred == 1:\n",
    "                predictions.append({\n",
    "                    \"prediction\": \"MATCH\",\n",
    "                    \"pmcid\": pmcid,\n",
    "                    \"pmid\": pmid,\n",
    "                    \"resource_name\": resource,\n",
    "                    \"matched_alias\": alias,\n",
    "                    \"sentence\": sentence,\n",
    "                    \"confidence\": probs[0, 1].item()\n",
    "                })\n",
    "            else:\n",
    "                predictions.append({\n",
    "                    \"prediction\": \"NO MATCH\",\n",
    "                    \"pmcid\": pmcid,\n",
    "                    \"pmid\": pmid,\n",
    "                    \"resource_name\": resource,\n",
    "                    \"matched_alias\": alias,\n",
    "                    \"sentence\": sentence,\n",
    "                    \"confidence\": probs[0, 0].item()\n",
    "                })\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTkYZG0cZGvc"
   },
   "source": [
    "# üß† Run Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 166,
     "status": "aborted",
     "timestamp": 1751462673130,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "CTm1B7PZUjVu"
   },
   "outputs": [],
   "source": [
    "# @title üìÑ Fetch and preprocess publication text\n",
    "\n",
    "epmc_query = f\"PMCID:{pmcid}\" if pmcid else f\"EXT_ID:{pmid}\"\n",
    "md = f\"- üîç Querying Europe PMC for {epmc_query}\\n\"\n",
    "data = query_europepmc(f\"{epmc_base_url}/search\", request_params={\n",
    "    'query': epmc_query,\n",
    "    'format': 'json',\n",
    "    'pageSize': 10,\n",
    "    'cursorMark': '*',\n",
    "    'resultType': 'core'\n",
    "})\n",
    "md += f\"- üîç Found {data.get('hitCount', 0)} results for {epmc_query}\\n\\n---\\n\"\n",
    "display(Markdown(md))\n",
    "\n",
    "for result in data.get('resultList', {}).get('result', []):\n",
    "    this_pmcid = result.get('pmcid')\n",
    "    this_pmid = result.get('pmid')\n",
    "    title = result.get('title')\n",
    "\n",
    "    # since we must use EXT_ID to search using PMID, this introduces room for error\n",
    "    # keep skipping through results until the match is found.\n",
    "    # In theory, we should only be processing 1 publication here.\n",
    "    if pmid and this_pmid != pmid:\n",
    "        # print(f\"‚ö†Ô∏è Skipping {this_pmcid} as it does not match the provided PMID {pmid}.\")\n",
    "        continue\n",
    "\n",
    "    md = f\"- üìÑ Title: {title}\\n\"\n",
    "    md += f\"- üÜî PMCID: {this_pmcid}, PMID: {this_pmid}\\n\\n---\\n\"\n",
    "    display(Markdown(md))\n",
    "\n",
    "    if pmcid:\n",
    "        # Get full text body and tables\n",
    "        text_body, table_blocks = get_fulltext_body(this_pmcid)\n",
    "        if not text_body:\n",
    "            print(\"‚ö†Ô∏è No full text body found.\")\n",
    "            continue\n",
    "    else:\n",
    "        text_body, table_blocks = sent_tokenize(result.get('abstractText')), []\n",
    "\n",
    "    break # only use the first successful match\n",
    "\n",
    "text_body = [tb.replace('\\n', ' ') for tb in text_body]\n",
    "text_body = [tb for tb in text_body if tb.strip()]\n",
    "\n",
    "md = f\"- Processed {len(text_body)} text blocks\\n\"\n",
    "md += f\"- Processed {len(table_blocks)} table blocks\\n\"\n",
    "display(Markdown(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 166,
     "status": "aborted",
     "timestamp": 1751462673131,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "V70fEJIzewG5"
   },
   "outputs": [],
   "source": [
    "# @title üßê Inspect text blocks (optional)\n",
    "# txb_df = pd.DataFrame(text_body, columns=[\"text_block\"])\n",
    "# txb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 258555,
     "status": "aborted",
     "timestamp": 1751462673143,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "1DNp10iZfQ1k"
   },
   "outputs": [],
   "source": [
    "# @title üßê Inspect table blocks (optional)\n",
    "# tb_df = pd.DataFrame(table_blocks, columns=[\"table_block\"])\n",
    "# tb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258567,
     "status": "aborted",
     "timestamp": 1751462673155,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "7N5KYNukbty3"
   },
   "outputs": [],
   "source": [
    "# @title üîç Search for resource mentions\n",
    "display(Markdown(f\"üîç Searching for resource mentions in {this_pmcid}...\"))\n",
    "mentions = get_resource_mentions(text_body, table_blocks, resource_names)\n",
    "display(Markdown(f\"üîç Found {len(mentions)} mentions of {len(set([x[2] for x in mentions]))} resources in {this_pmcid}.\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 258567,
     "status": "aborted",
     "timestamp": 1751462673156,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "tB4Gi1OVduNU"
   },
   "outputs": [],
   "source": [
    "# @title üßê Inspect unclassified mentions (optional)\n",
    "mentions_df = pd.DataFrame(mentions, columns=[\"sentence\", \"alias\", \"resource_name\"])\n",
    "mentions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258567,
     "status": "aborted",
     "timestamp": 1751462673156,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "IoqjebMQcTtX"
   },
   "outputs": [],
   "source": [
    "# @title üß† Classify resource mentions\n",
    "classified_mentions = classify_mentions(this_pmcid, this_pmid, mentions)\n",
    "\n",
    "class_df = pd.DataFrame(classified_mentions)\n",
    "class_df.sort_values(by=['prediction', 'confidence'], ascending=[False, False], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258566,
     "status": "aborted",
     "timestamp": 1751462673156,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "6qMuptgwXXuM"
   },
   "outputs": [],
   "source": [
    "class_df_display = class_df[['resource_name', 'matched_alias', 'prediction', 'pmcid', 'sentence']].copy()\n",
    "class_df_display['prediction'] = class_df_display['prediction'].map({'MATCH': 1, 'NO MATCH': 0})\n",
    "data_table.DataTable(class_df_display, include_index=False, num_rows_per_page=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5i1gMIpcr8g"
   },
   "source": [
    "## üìä View results by confidence bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258566,
     "status": "aborted",
     "timestamp": 1751462673157,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "kuwU9k_agthy"
   },
   "outputs": [],
   "source": [
    "# @title ‚≠ê High confidence (> 0.98)\n",
    "high = class_df[class_df[\"confidence\"] > 0.98]\n",
    "high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258565,
     "status": "aborted",
     "timestamp": 1751462673157,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "diqRROgWcKhD"
   },
   "outputs": [],
   "source": [
    "# @title üìò Medium-high confidence (0.9‚Äì0.98)\n",
    "mid_high = class_df[(class_df[\"confidence\"] <= 0.98) & (class_df[\"confidence\"] > 0.9)]\n",
    "mid_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258565,
     "status": "aborted",
     "timestamp": 1751462673158,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "-OFit2gLhBTB"
   },
   "outputs": [],
   "source": [
    "# @title üìí Medium-low confidence (0.8‚Äì0.9)\n",
    "mid_low = class_df[(class_df[\"confidence\"] <= 0.9) & (class_df[\"confidence\"] > 0.8)]\n",
    "mid_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258565,
     "status": "aborted",
     "timestamp": 1751462673158,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "BOM3yazQhJFh"
   },
   "outputs": [],
   "source": [
    "# @title ‚ö†Ô∏è Low confidence (‚â§ 0.8)\n",
    "low = class_df[class_df[\"confidence\"] <= 0.8]\n",
    "low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohdoNhTwtIyH"
   },
   "source": [
    "## üèÅ Publication Classification Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 258564,
     "status": "aborted",
     "timestamp": 1751462673158,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "6chMbNlOtGbM"
   },
   "outputs": [],
   "source": [
    "# @title ‚ùì Does this publication have biodata resource mentions?\n",
    "\n",
    "summary_df = (\n",
    "    class_df[(class_df['prediction'] == 'MATCH') & (class_df['confidence'] >= 0.9)]\n",
    "    .groupby('resource_name', as_index=False)\n",
    "    .agg({\n",
    "        'confidence': 'mean',\n",
    "        'prediction': 'count',\n",
    "        'sentence': lambda x: \" || \".join(list(set(x)))  # unique sentences\n",
    "    })\n",
    ")\n",
    "summary_df.rename(columns={'confidence': 'mean_confidence', 'sentence':'token_matches', 'prediction': 'num_matches'}, inplace=True)\n",
    "\n",
    "if len(summary_df) > 0:\n",
    "    result_md = f\"## ‚úÖ Publication _has_ verified known biodata resource mention(s)\\n\\n---\\n\"\n",
    "    result_md += f\"### Resource Match Summary\\n\"\n",
    "    result_md += summary_df[['resource_name', 'num_matches', 'mean_confidence']].to_markdown(index=False)\n",
    "\n",
    "else:\n",
    "    result_md = f\"## ‚ùå Publication _does not_ mention a known biodata resource\"\n",
    "\n",
    "display(Markdown(result_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQRybFQ4_ean"
   },
   "source": [
    "# ü§ñ Test Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258565,
     "status": "aborted",
     "timestamp": 1751462673159,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "yCFGAive_gWy"
   },
   "outputs": [],
   "source": [
    "# text_blocks, table_blocks = get_fulltext_body(pmcid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258563,
     "status": "aborted",
     "timestamp": 1751462673159,
     "user": {
      "displayName": "Carla Cummins",
      "userId": "04681454191639308511"
     },
     "user_tz": -60
    },
    "id": "Hv1UwlqYR1rT"
   },
   "outputs": [],
   "source": [
    "# display(JSON(text_blocks))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNMCwx0SEfO3BfjSUwffuqx",
   "collapsed_sections": [
    "pmq65xvwXYxT"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
